一、ReLU的主要变体
1. Leaky ReLU

公式: f(x) = max(αx, x)，其中α是一个很小的常数（如0.01）。

思想: 在负区间赋予一个很小的斜率α，使得负输入也有一个非零梯度，从而允许权重更新，避免了神经元坏死。

优点: 解决了“死亡ReLU”问题，保持了计算的高效性。

2. Parametric ReLU (PReLU)

公式: f(x) = max(αx, x)，但斜率α作为可学习的参数，在训练中与网络权重一同优化。

思想: 将Leaky ReLU的固定斜率变为自适应参数，让网络自己决定每个通道的最佳负区间斜率。

优点: 比Leaky ReLU更灵活，性能可能更优，但引入了少量额外参数。

3. Exponential Linear Unit (ELU)

公式:

f(x) = x, if x > 0

f(x) = α(exp(x) - 1), if x ≤ 0

思想:

在负区间使用一个趋近于-α的饱和曲线（而非直线），使输出均值更接近0，加速收敛。

在负值部分平滑可导，缓解了ReLU在零点处的非平滑问题。

优点: 缓解了神经元坏死，具有更接近零均值的输出，对噪声更鲁棒。

缺点: 计算涉及指数运算，比ReLU慢。

4. Scaled Exponential Linear Unit (SELU)

公式: f(x) = λ * ELU(x)，其中λ是一个精确的缩放常数（约1.0507）。

思想: 经过严谨的数学推导，当网络权重按特定方式初始化时，SELU具有自归一化的特性。即在前向传播中，输出的均值和方差会保持稳定，从而防止梯度爆炸或消失。

优点: 特别适合深层全连接网络，无需批归一化也能稳定训练。

缺点: 必须与特定的权重初始化方法配合使用，且理论复杂。

5. Gaussian Error Linear Unit (GELU)

公式: f(x) = x * Φ(x)，其中Φ(x)是标准高斯分布的累积分布函数。常用近似：0.5x * (1 + tanh[sqrt(2/π)(x + 0.044715x^3)])

思想: 基于随机正则化的思想。神经元的输出是否被“激活”，取决于输入相对于其他输入的“概率”。它不是一个硬性的0/1门控（如ReLU），而是一个随输入大小变化的软门控。

优点: 在自然语言处理领域表现卓越，被BERT、GPT等Transformer架构广泛采用，被认为比ReLU更平滑、更符合概率建模。

缺点: 计算相对复杂。

二、 其他重要的激活函数
1. Swish

公式: f(x) = x * sigmoid(βx)，其中β可以是常数或可学习参数。

思想: 受LSTM和GRU中门的启发，是一种自门控激活函数。它结合了线性函数和Sigmoid的平滑门控特性。

特点: 在多个视觉任务上，表现优于或媲美ReLU。是无上界、有下界、平滑、非单调的函数。

缺点: 计算量比ReLU大。

2. Mish

公式: f(x) = x * tanh(softplus(x))，其中softplus(x) = ln(1 + exp(x))

思想: 类似于Swish，是一个自正则化的非单调激活函数。

特点: 在计算机视觉的多个任务（尤其是目标检测）中表现出比Swish和ReLU更好的性能。非常平滑的梯度流有助于信息深度传播。

缺点: 计算成本最高。


